
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

   <meta http-equiv="content-type" content="text/html;charset=UTF-8">

   <style type="text/css">

   /*
CSS stylesheet is based on killwing's flavored markdown style:
https://gist.github.com/2937864
*/
body{
    margin: 0 auto;
    font: 13px/1.231 Helvetica, Arial, sans-serif;
    color: #444444;
    line-height: 1;
    max-width: 960px;
    padding: 5px;
}
h1, h2, h3, h4 {
    color: #111111;
    font-weight: 400;
}
h1, h2, h3, h4, h5, p {
    margin-bottom: 16px;
    padding: 0;
}
h1 {
    font-size: 28px;
}
h2 {
    font-size: 22px;
    margin: 20px 0 6px;
}
h3 {
    font-size: 21px;
}
h4 {
    font-size: 18px;
}
h5 {
    font-size: 16px;
}
a {
    color: #0099ff;
    margin: 0;
    padding: 0;
    vertical-align: baseline;
}
a:link,a:visited{
 text-decoration:none;
}
a:hover{
 text-decoration:underline;
}
ul, ol {
    padding: 0;
    margin: 0;
}
li {
    line-height: 24px;
    margin-left: 44px;
}
li ul, li ul {
    margin-left: 24px;
}
ul, ol {
    font-size: 14px;
    line-height: 20px;
    max-width: 540px;
}

p {
    font-size: 14px;
    line-height: 20px;
    max-width: 540px;
    margin-top: 3px;
}

pre {
    padding: 0px 4px;
    max-width: 800px;
    white-space: pre-wrap;
    font-family: Consolas, Monaco, Andale Mono, monospace;
    line-height: 1.5;
    font-size: 13px;
    border: 1px solid #ddd;
    background-color: #f7f7f7;
    border-radius: 3px;
}
code {
    font-family: Consolas, Monaco, Andale Mono, monospace;
    line-height: 1.5;
    font-size: 13px;
    border: 1px solid #ddd;
    background-color: #f7f7f7;
    border-radius: 3px;
}
pre code {
    border: 0px;
}
aside {
    display: block;
    float: right;
    width: 390px;
}
blockquote {
    border-left:.5em solid #40AA53;
    padding: 0 2em;
    margin-left:0;
    max-width: 476px;
}
blockquote  cite {
    font-size:14px;
    line-height:20px;
    color:#bfbfbf;
}
blockquote cite:before {
    content: '\2014 \00A0';
}

blockquote p {  
    color: #666;
    max-width: 460px;
}
hr {
    height: 1px;
    border: none;
    border-top: 1px dashed #0066CC
}

button,
input,
select,
textarea {
  font-size: 100%;
  margin: 0;
  vertical-align: baseline;
  *vertical-align: middle;
}
button, input {
  line-height: normal;
  *overflow: visible;
}
button::-moz-focus-inner, input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
button,
input[type="button"],
input[type="reset"],
input[type="submit"] {
  cursor: pointer;
  -webkit-appearance: button;
}
input[type=checkbox], input[type=radio] {
  cursor: pointer;
}
/* override default chrome & firefox settings */
input:not([type="image"]), textarea {
  -webkit-box-sizing: content-box;
  -moz-box-sizing: content-box;
  box-sizing: content-box;
}

input[type="search"] {
  -webkit-appearance: textfield;
  -webkit-box-sizing: content-box;
  -moz-box-sizing: content-box;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
label,
input,
select,
textarea {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  font-weight: normal;
  line-height: normal;
  margin-bottom: 18px;
}
input[type=checkbox], input[type=radio] {
  cursor: pointer;
  margin-bottom: 0;
}
input[type=text],
input[type=password],
textarea,
select {
  display: inline-block;
  width: 210px;
  padding: 4px;
  font-size: 13px;
  font-weight: normal;
  line-height: 18px;
  height: 18px;
  color: #808080;
  border: 1px solid #ccc;
  -webkit-border-radius: 3px;
  -moz-border-radius: 3px;
  border-radius: 3px;
}
select, input[type=file] {
  height: 27px;
  line-height: 27px;
}
textarea {
  height: auto;
}

/* grey out placeholders */
:-moz-placeholder {
  color: #bfbfbf;
}
::-webkit-input-placeholder {
  color: #bfbfbf;
}

input[type=text],
input[type=password],
select,
textarea {
  -webkit-transition: border linear 0.2s, box-shadow linear 0.2s;
  -moz-transition: border linear 0.2s, box-shadow linear 0.2s;
  transition: border linear 0.2s, box-shadow linear 0.2s;
  -webkit-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);
  -moz-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);
}
input[type=text]:focus, input[type=password]:focus, textarea:focus {
  outline: none;
  border-color: rgba(82, 168, 236, 0.8);
  -webkit-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1), 0 0 8px rgba(82, 168, 236, 0.6);
  -moz-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1), 0 0 8px rgba(82, 168, 236, 0.6);
  box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1), 0 0 8px rgba(82, 168, 236, 0.6);
}

/* buttons */
button {
  display: inline-block;
  padding: 4px 14px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 18px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  border-radius: 4px;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.2), 0 1px 2px rgba(0, 0, 0, 0.05);
  -moz-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.2), 0 1px 2px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.2), 0 1px 2px rgba(0, 0, 0, 0.05);
  background-color: #0064cd;
  background-repeat: repeat-x;
  background-image: -khtml-gradient(linear, left top, left bottom, from(#049cdb), to(#0064cd));
  background-image: -moz-linear-gradient(top, #049cdb, #0064cd);
  background-image: -ms-linear-gradient(top, #049cdb, #0064cd);
  background-image: -webkit-gradient(linear, left top, left bottom, color-stop(0%, #049cdb), color-stop(100%, #0064cd));
  background-image: -webkit-linear-gradient(top, #049cdb, #0064cd);
  background-image: -o-linear-gradient(top, #049cdb, #0064cd);
  background-image: linear-gradient(top, #049cdb, #0064cd);
  color: #fff;
  text-shadow: 0 -1px 0 rgba(0, 0, 0, 0.25);
  border: 1px solid #004b9a;
  border-bottom-color: #003f81;
  -webkit-transition: 0.1s linear all;
  -moz-transition: 0.1s linear all;
  transition: 0.1s linear all;
  border-color: #0064cd #0064cd #003f81;
  border-color: rgba(0, 0, 0, 0.1) rgba(0, 0, 0, 0.1) rgba(0, 0, 0, 0.25);
}
button:hover {
  color: #fff;
  background-position: 0 -15px;
  text-decoration: none;
}
button:active {
  -webkit-box-shadow: inset 0 3px 7px rgba(0, 0, 0, 0.15), 0 1px 2px rgba(0, 0, 0, 0.05);
  -moz-box-shadow: inset 0 3px 7px rgba(0, 0, 0, 0.15), 0 1px 2px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 3px 7px rgba(0, 0, 0, 0.15), 0 1px 2px rgba(0, 0, 0, 0.05);
}
button::-moz-focus-inner {
  padding: 0;
  border: 0;
}
/* table  */
table {
    border-spacing: 0;
    border: 1px solid #ccc;
}
td, th{
    border: 1px solid #ccc;
    padding: 5px;
}
/* code syntax highlight.
Documentation: http://www.mdcharm.com/documentation/code_syntax_highlighting.html#custom_your_own
 */
pre .literal,
pre .comment,
pre .template_comment,
pre .diff .header,
pre .javadoc {
    color: #008000;
}

pre .keyword,
pre .css .rule .keyword,
pre .winutils,
pre .javascript .title,
pre .nginx .title,
pre .subst,
pre .request,
pre .status {
    color: #0000FF;
    font-weight: bold
}

pre .number,
pre .hexcolor,
pre .python .decorator,
pre .ruby .constant {
    color: #0000FF;
}

pre .string,
pre .tag .value,
pre .phpdoc,
pre .tex .formula {
    color: #D14
}

pre .title,
pre .id {
    color: #900;
    font-weight: bold
}

pre .javascript .title,
pre .lisp .title,
pre .clojure .title,
pre .subst {
    font-weight: normal
}

pre .class .title,
pre .haskell .type,
pre .vhdl .literal,
pre .tex .command {
    color: #458;
    font-weight: bold
}

pre .tag,
pre .tag .title,
pre .rules .property,
pre .django .tag .keyword {
    color: #000080;
    font-weight: normal
}

pre .attribute,
pre .variable,
pre .lisp .body {
    color: #008080
}

pre .regexp {
    color: #009926
}

pre .class {
    color: #458;
    font-weight: bold
}

pre .symbol,
pre .ruby .symbol .string,
pre .lisp .keyword,
pre .tex .special,
pre .prompt {
    color: #990073
}

pre .built_in,
pre .lisp .title,
pre .clojure .built_in {
    color: #0086b3
}

pre .preprocessor,
pre .pi,
pre .doctype,
pre .shebang,
pre .cdata {
    color: #999;
    font-weight: bold
}

pre .deletion {
    background: #fdd
}

pre .addition {
    background: #dfd
}

pre .diff .change {
    background: #0086b3
}

pre .chunk {
    color: #aaa
}

pre .markdown .header {
    color: #800;
    font-weight: bold;
}

pre .markdown .blockquote {
    color: #888;
}

pre .markdown .link_label {
    color: #88F;
}

pre .markdown .strong {
    font-weight: bold;
}

pre .markdown .emphasis {
    font-style: italic;
}

   </style>

   

   

</head>

<body>

    <h1><strong>Song Wenfeng Home</strong></h1>

<h1></h1>

<p>(宋文凤)<br>
Associate Professfor<br>
Beijing Information Science & Technology Univeristy<br>
Virtual Reality & Computer Vision<br>
Email: songwenfenga@gmail.com<br>
<!-- Tel: 18810318719<br> -->
Github: <a href="https://github.com/buaaswf">https://github.com/buaaswf</a> <br>
CSDN: <a href="https://blog.csdn.net/swfa1">https://blog.csdn.net/swfa1</a>

<p><img src='./zhengjianzhao.jpg', width='150'></p>



<h2>Short Bio</h2>

<p>My research interests include virtual reality and computer vision with medical applications (e.g., human-scene interaction, medical image analysis) especially in the field of virtual human reconstruction. The experience in the State Key Laboratory of Virtual Reality Technology and System of Beihang motivates my interest and deep understanding of deep learning on Computer-assisted Diagnosis (CAD). My research domain is virtual reality for medical education and environment perception based on deep learning, including object detection based on image and video including unbalance dataset, depth estimation, person reid, UAV tracking. Here is the link of my  <a href="1.pdf">CV</a>. </p>



<p><strong>If you are interested in my topics , please send me an <a href="mailto:songwenfenga@gmail.com">email</a>.</strong></p>


<h2>Education</h2>



<!-- --> 

<blockquote>
<p>Beihang University</a> <br />
Phd. Degree, Computer Science <br />
From Sept. 2012 to 2020 <br /></p>
</blockquote>

<!-- --> 



<h2>Research</h2>

<ul>

<li>Scene reconstruction and generation</li>
<li> Vitual human generation</li>
<li>Computer aided medical</li>



</ul>




<!-- <h2>Publications</h2>

<ul>

<li>Have in-depth research on scene understanding and environment perception based on deep learning, including object detection based on vision and LiDAR, scene segmentation and other perception tasks;</li>

<li>Perfect in designing the network structure according to the needs of the real scene, and at the same time, can also ensure the algorithm is both scalable and commercializing feasible；</li>
</ul> -->



<h2>Cooperators</h2>
<ul>


<li>Prof.Li Shuai<a href="http://scse.buaa.edu.cn/info/1079/2733.html/"></a>, Prof.Qin Hong<a href="https://www3.cs.stonybrook.edu/~qin/"></a>, Prof.Hao Aimin<a href="http://scse.buaa.edu.cn/info/1078/2654.html"></a>, Fang Zheng, Shi Jiaying, Liu Ji, Wang Jianxiong</li>

</ul>



<h2>Experience</h2>

<ul>

<li>Beijing Information Science and Technology Univeristy, 2020</li>
<li>BUAA Virtual Reality Laboratory，2015</li>
<li>BUAA Software College，2012</li>
</ul>


<h2>Publications</h2>

<ul>

<li>Li S, Shi J, Song W (*Corresponding author), et al. Hierarchical Object Relationship Constrained Monocular Depth Estimation[J]. Pattern Recognition, 2021, 120: 108116.</li>
<li>Deng S, Li S, Xie K, et al. A global-local self-adaptive network for drone-view object detection[J]. IEEE Transactions on Image Processing, 2020, 30: 1556-1569.</li>
<li>Wenfeng Song, Shuai Li and et al (2018). Multitask Cascade Convolution Neural Networks for Automatic Thyroid Nodule Detection and Recognition. IEEE journal of biomedical and health informatics, 23(3), 1215-1224. (JCR 1区impact factor 4.21)</li>
<li>Wenfeng Song, Shuai Li, JiLiu, Hong Qin and Aimin. Hao Contextualized CNN for Scene-aware Depth Estimation from Single RGB Image [J]. IEEE Transactions on Multimedia (CCF B，JCR 1区，impact factor 5.45)</li>
<li>Shuai Li (导师), Wenfeng Song, Hong Qin, Aimin Hao: Deep variance network: An iterative, improved CNN framework for unbalanced training datasets. Pattern Recognition 81: 294-308 (2018) (CCF B, JCR 1区, impact factor 5.90)</li>
</ul>


<h2>Selected Project</h2>
<p></p>
<ul>

  
<li> <p><strong>Automatic Thyroid Nodule Detection and Recognition System</strong></p><strong>[Abstract]</strong> To alleviate doctors’
tremendous labor in the diagnosis procedure, we advocate
a machine learning approach to the detection and recognition tasks in this paper. In particular, we develop a multitask
cascade convolution neural network (MC-CNN) framework
to exploit the context information of thyroid nodules. It may
be noted that our framework is built upon a large number of
clinically confirmed thyroid ultrasound images with accurate and detailed ground truth labels. Other key advantages
of our framework result from a multitask cascade architecture, two stages of carefully designed deep convolution networks in order to detect and recognize thyroid nodules in a
pyramidal fashion, and capturing various intrinsic features
in a global-to-local way.<p><img src='./images/1.jpg', width='350'></p></li>
<p></p>
<li><p><strong>Unbalance dataset for CNN </strong></p><strong>[Abstract]</strong> The key idea of our DVN is built upon the intrinsic exploitation of inter-class homogeneity and intra-class heterogeneity. Towards such goal, we make the first attempt to incorporate a hierarchical Bayesian model into the powerful CNN framework, which can transfer the joint feature distribution from certain object’s complete training dataset to other object’s incomplete training dataset in an iterative way. In each training cycle, the CNN-resulted features are clustered into discrimination-related subspaces to guide the learning and adaptive adjustment of homogeneity and heterogeneity over unbalanced training datasets. In practice, we furnish several state-of-the-art deep networks with our proposed DVN, and conduct extensive experiments and comprehensive evaluations over CIFAR-10, MNIST, and SVHN benchmarks.<p><img src='./images/2.jpg', width='350'></p></li>
<p></p>
<li><p><strong>Depth Estimation</strong></p><strong>[Coming Soon]</strong><p><img src='./images/3.jpg', width='350'></p></li>
<p></p>
<li><p><strong>Person ReID<strong></p>[Coming Soon]</strong> <p><img src='./images/4.jpg', width='350'></p></li>
<li><p><strong>Tracking<p><strong></p>[Coming Soon]</strong>  <p><img src='./images/5.jpg', width='350'></p></li>


</ul>






<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-82196129-1', 'auto');
  ga('send', 'pageview');

</script>


</body>

</html>
